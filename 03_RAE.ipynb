{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Seung-hwanSong/Anomaly.git #코랩 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [시계열 이상치 탐지 Part 2]\n",
    "## Recurrunt Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### jupyter notebook 단축키\n",
    "\n",
    "- ctrl+enter: 셀 실행   \n",
    "- shift+enter: 셀 실행 및 다음 셀 이동   \n",
    "- alt+enter: 셀 실행, 다음 셀 이동, 새로운 셀 생성\n",
    "- a: 상단에 새로운 셀 만들기\n",
    "- b: 하단에 새로운 셀 만들기\n",
    "- dd: 셀 삭제(x: 셀 삭제)\n",
    "- 함수 ( ) 안에서 shift+tab: arguments description. shift+tab+tab은 길게 볼 수 있도록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 데이터 전처리 패키지 '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "''' 기계학습 모델 구축 및 평가 패키지 '''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "''' 데이터 시각화 패키지 '''\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 불러오기: NASA Bearing Dataset\n",
    "\n",
    "- 데이터 description <br>\n",
    "    - NASA Bearing Dataset은 NSF I/UCR Center의 Intelligent Maintenance System의 4개의 bearing에서 고장이 발생할 때까지 10분 단위로 수집된 센서 데이터이다. 본 데이터셋은 특정 구간에서 기록된 1-second vibration signal snapshots을 나타내는 여러 개의 파일로 구성되어 있다. 각 파일은 20 kHz 단위로 샘플링 된 20,480개의 data point를 포함하고 있으며, 각 파일의 이름은 데이터가 수집된 시간을 의미한다. 해당 데이터셋은 크게 3개의 데이터를 포함하고 있으며, 본 실습에서 사용하는 데이터는 bearing 1에서 outer race failure가 발생할 때까지 수집된 센서 데이터이다. <br><br>\n",
    "- 변수\n",
    "    - 센서 데이터: Bearing1, Bearing2, Bearing3, Bearing4 <br><br>\n",
    "- 출처: https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "# data = pd.read_csv('/content/Anomaly/data/nasa_bearing_dataset.csv', index_col=0)\n",
    "data = pd.read_csv('./data/nasa_bearing_dataset.csv', index_col=0)\n",
    "\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. 데이터 Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[data['data_type'] == 'train'].iloc[:, :4]\n",
    "\n",
    "X_test = data[data['data_type'] == 'test'].iloc[:, :4]\n",
    "y_test = data[data['data_type'] == 'test'].iloc[:, -2].values\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 기반으로 train/test 데이터에 대하여 min-max scaling 적용 \n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rae_X_train = scaler.transform(X_train)\n",
    "rae_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Autoencoder\n",
    ">입력과 출력이 동일한 인공신경망 구조 <br>\n",
    ">정상 데이터에 대한 학습이 충분히 되어 있을 경우 정상 데이터는 자기 자신을 잘 복원할 수 있지만, 이상치 제이터는 학습 기회가 적어 상대적으로 잘 복원하지 못할 것을 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/KpyS57D.jpg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencdoer 사용을 위한 torch 모듈 불러오기\n",
    "\n",
    "import copy\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, roc_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter 설정\n",
    "seq_len = 5\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "\n",
    "random_seed = 2024\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Detect if we have a GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. DataLoader 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_series_dataset(X_train, X_test, y_test, seq_len):\n",
    "    # train/test 데이터를 seq_len 시점 길이로 분할\n",
    "    datasets = []\n",
    "    for X in [X_train, X_test]:\n",
    "        T = X.shape[0]\n",
    "        windows = np.array([X[i:i + seq_len] for i in range(T - seq_len + 1)])\n",
    "        datasets.append(windows)\n",
    "\n",
    "    X_train, X_test = datasets[0], datasets[1]\n",
    "    \n",
    "    # 각 window의 마지막 시점의 label을 해당 window의 label로 사용\n",
    "    y_test = np.array([y_test[i + seq_len - 1] for i in range(T - seq_len + 1)])\n",
    "    \n",
    "    # validation/test data 분할\n",
    "    valid_idx = set(np.random.randint(0, len(X_test), int(len(X_test) * 0.4)))\n",
    "    X_valid = np.array([X_test[i] for i in valid_idx])\n",
    "    y_valid = np.array([y_test[i] for i in valid_idx])\n",
    "    \n",
    "    test_idx = set(np.arange(len(X_test))) - valid_idx\n",
    "    X_test = np.array([X_test[i] for i in test_idx])\n",
    "    y_test = np.array([y_test[i] for i in test_idx])\n",
    "    \n",
    "    return X_train, X_valid, X_test, y_valid, y_test, list(valid_idx), list(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = make_time_series_dataset(rae_X_train, rae_X_test, y_test, seq_len)\n",
    "rae_X_train, rae_X_valid, rae_X_test, y_valid, y_test, valid_idx, test_idx = Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[seq_len - 1:, ]\n",
    "X_valid = X_test.iloc[valid_idx, :]\n",
    "X_test = X_test.iloc[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test dataloader 생성\n",
    "rae_train_dataset = torch.utils.data.TensorDataset(torch.Tensor(rae_X_train))\n",
    "rae_train_loader = torch.utils.data.DataLoader(rae_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "rae_test_dataset = torch.utils.data.TensorDataset(torch.Tensor(rae_X_test))\n",
    "rae_test_loader = torch.utils.data.DataLoader(rae_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataloader의 경우 train/test 단계에서 사용될 loader를 따로 생성함\n",
    "# train 단계: 정상으로만 구성, test 단계: 정상과 비정상으로 구성\n",
    "rae_X_valid_train = rae_X_valid[y_valid == 0]\n",
    "rae_valid_train_dataset = torch.utils.data.TensorDataset(torch.Tensor(rae_X_valid_train))\n",
    "rae_valid_train_loader = torch.utils.data.DataLoader(rae_valid_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "rae_valid_test_dataset = torch.utils.data.TensorDataset(torch.Tensor(rae_X_valid))\n",
    "rae_valid_test_loader = torch.utils.data.DataLoader(rae_valid_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 모델 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seq_len):\n",
    "        super(RecurrentAutoEncoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size = hidden_size,\n",
    "            hidden_size = 2 * hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(2 * hidden_size, input_size)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        enc_x, (enc_hidden_state, enc_cell_state) = self.encoder(x)\n",
    "        enc_last_hidden = enc_hidden_state[-1, :, :]\n",
    "        \n",
    "        enc_last_hidden = enc_last_hidden.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "        dec_x, (dec_hidden_state, dec_cell_state) = self.decoder(enc_last_hidden)\n",
    "        dec_x = dec_x.reshape((x.size(0), self.seq_len, -1))\n",
    "        \n",
    "        out = self.fc(dec_x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rae_model = RecurrentAutoEncoder(input_size, hidden_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Train 데이터로 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model 학습\n",
    "def train_model(dataloaders, model, criterion, num_epochs, learning_rate, device):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_valid_loss = 10000000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == 0 or (epoch + 1) % 50 == 0:\n",
    "            print()\n",
    "            print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_total = 0\n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for i, inputs in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs[0].to(device)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_total += inputs.size(0)\n",
    "\n",
    "            # epoch의 loss 및 accuracy 도출\n",
    "            epoch_loss = running_loss / running_total\n",
    "\n",
    "            if epoch == 0 or (epoch + 1) % 50 == 0:\n",
    "                print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "            # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "            if phase == 'val' and epoch_loss < best_valid_loss:\n",
    "                best_valid_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    print('Best validation loss: {:4f}'.format(best_valid_loss))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rae_dataloaders = {'train': rae_train_loader, 'val': rae_valid_train_loader}\n",
    "rae_train_criterion = nn.MSELoss()\n",
    "rae_model = train_model(rae_dataloaders, rae_model, rae_train_criterion, num_epochs, learning_rate, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. 적합된 모델을 기반으로 train/test 데이터의 anomaly score 도출 (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 검증\n",
    "def test_model(data_loader, model, criterion, device):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        for i, inputs in enumerate(data_loader):\n",
    "            inputs = inputs[0].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss = loss.reshape(loss.shape[0], -1).mean(axis=1)\n",
    "\n",
    "            test_loss += list(loss.data.cpu().numpy())\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train/validation/test 데이터에 대한 최종 결과 도출\n",
    "rae_test_criterion = nn.L1Loss(reduction='none')\n",
    "rae_train_loader = torch.utils.data.DataLoader(rae_train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "rae_train = test_model(rae_train_loader, rae_model, rae_test_criterion, device)\n",
    "rae_valid = test_model(rae_valid_test_loader, rae_model, rae_test_criterion, device)\n",
    "rae_test = test_model(rae_test_loader, rae_model, rae_test_criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation/test 데이터의 anomaly score 분포 시각화\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize = (12, 8))\n",
    "\n",
    "sns.distplot(rae_train, bins=100, kde=True, color='blue', ax=ax1)\n",
    "sns.distplot(rae_valid, bins=100, kde=True, color='green', ax=ax2)\n",
    "sns.distplot(rae_test, bins=100, kde=True, color='red', ax=ax3)\n",
    "ax1.set_title(\"Train Data\")\n",
    "ax2.set_title(\"Validation Data\")\n",
    "ax3.set_title(\"Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Threshold 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold 탐색\n",
    "# score의 min ~ max 범위를 num_step개로 균등 분할한 threshold에 대하여 best threshold 탐색 \n",
    "def search_best_threshold(score, y_true, num_step):\n",
    "    best_f1 = 0\n",
    "    best_threshold = None\n",
    "    for threshold in np.linspace(min(score), max(score), num_step):\n",
    "        y_pred = threshold < score\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print('Best threshold: ', round(best_threshold, 4))\n",
    "    print('Best F1 Score:', round(best_f1, 4))\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# best threshold 탐색\n",
    "rae_best_threshold = search_best_threshold(rae_valid, y_valid, num_step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Best threshold를 기반으로 이상치 탐지 모형 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 최종 결과 도출\n",
    "rae_scores = pd.DataFrame(columns=['score', 'anomaly'])\n",
    "for date, score in zip([X_train.index, X_valid.index, X_test.index], [rae_train, rae_valid, rae_test]):\n",
    "    rae_score = pd.DataFrame(index=date)\n",
    "    rae_score['score'] = score\n",
    "    rae_score['anomaly'] = rae_best_threshold < score\n",
    "    rae_scores = rae_scores.append(rae_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rae_scores = rae_scores.sort_index()\n",
    "rae_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# anomaly score plot 도출\n",
    "def draw_plot(scores, threshold):\n",
    "    normal_scores = scores[scores['anomaly'] == False]\n",
    "    abnormal_scores = scores[scores['anomaly'] == True]\n",
    "\n",
    "    plt.figure(figsize = (12,5))\n",
    "    plt.scatter(normal_scores.index, normal_scores['score'], label='Normal', c='blue', s=3)\n",
    "    plt.scatter(abnormal_scores.index, abnormal_scores['score'], label='Abnormal', c='red', s=3)\n",
    "    \n",
    "    plt.axhline(threshold, c='green', alpha=0.7)\n",
    "    plt.axvline(data.index[int(len(data) * 0.5)], c='orange', ls='--')\n",
    "    \n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 기간의 데이터 분포 확인\n",
    "draw_plot(rae_scores, rae_best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRR, FAR, F1 score, AUROC 도출\n",
    "def calculate_metric(y_true, y_pred):\n",
    "    # FRR, FAR\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[True, False])\n",
    "    tp, fn, fp, tn = cm.ravel()\n",
    "    frr = fp / (fp + tn)\n",
    "    far = fn / (fn + tp) \n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # AUROC, IE\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    \n",
    "    return frr, far, f1, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 평가 지표 결과 확인\n",
    "frr, far, f1, auroc = calculate_metric(y_test, rae_best_threshold < rae_test)\n",
    "\n",
    "print(\"**  FRR: {}  |  FAR: {}  |  F1 Score: {}  |  AUROC: {}\"\n",
    "      .format(round(frr, 4), round(far, 4), round(f1, 4), round(auroc, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttest3.8",
   "language": "python",
   "name": "ttest3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
